{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Recreating Experiments: FS_MedSAM2\n",
    "\n",
    "This notebook documents the process of recreating the experiments for the FS_MedSAM2 project. The following sections will guide you through the data preparation, analysis, and visualization steps involved in this experiment.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/FS_MedSAM2'...\n",
      "remote: Enumerating objects: 76, done.\u001b[K\n",
      "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
      "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
      "remote: Total 76 (delta 39), reused 52 (delta 28), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (76/76), 6.54 MiB | 8.80 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#!git clone https://github.com/cheng-01037/Self-supervised-Fewshot-Medical-Image-Segmentation.git /SSL_ALPNet\n",
    "\n",
    "!git clone https://github.com/DeepMed-Lab-ECNU/FS_MedSAM2.git /FS_MedSAM2\n",
    "# Define the paths to be replaced\n",
    "saved_npz_old = \"/path/to/saved_npz\"\n",
    "saved_npz_new = \"/FS_MedSAM2/example_data\"\n",
    "ckpt_old = \"/path/to/ckpt\"\n",
    "ckpt_new = \"/opt/sam2/checkpoints\"\n",
    "\n",
    "# Directory containing the .py files\n",
    "notebooks_dir = \"/FS_MedSAM2/notebooks\"\n",
    "\n",
    "# Iterate over each .py file in the directory\n",
    "for filename in os.listdir(notebooks_dir):\n",
    "    if filename.endswith(\".py\"):\n",
    "        filepath = os.path.join(notebooks_dir, filename)\n",
    "        \n",
    "        # Read the file\n",
    "        with open(filepath, 'r') as file:\n",
    "            filedata = file.read()\n",
    "        \n",
    "        # Replace the target strings\n",
    "        filedata = filedata.replace(saved_npz_old, saved_npz_new)\n",
    "        filedata = filedata.replace(ckpt_old, ckpt_new)\n",
    "        \n",
    "        # Write the file out again\n",
    "        with open(filepath, 'w') as file:\n",
    "            file.write(filedata)\n",
    "\n",
    "!cp /FS_MedSAM2/sam2/utils/* /opt/sam2/sam2/utils/\n",
    "!cp /FS_MedSAM2/sam2/*.py /opt/sam2/sam2/\n",
    "!cp -r /FS_MedSAM2/notebooks/utils/ /opt/sam2/notebooks/utils/\n",
    "!cp /FS_MedSAM2/notebooks/*.py /opt/sam2/notebooks/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "/opt/sam2/sam2/modeling/sam/transformer.py:270: UserWarning: Memory efficient kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:719.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "/opt/sam2/sam2/modeling/sam/transformer.py:270: UserWarning: Memory Efficient attention has been runtime disabled. (Triggered internally at ../aten/src/ATen/native/transformers/sdp_utils_cpp.h:495.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "/opt/sam2/sam2/modeling/sam/transformer.py:270: UserWarning: Flash attention kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:721.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "/opt/sam2/sam2/modeling/sam/transformer.py:270: UserWarning: Flash attention was not compiled for current AMD GPU architecture. Attempting to run on architecture gfx1030 (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:193.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "/opt/sam2/sam2/modeling/sam/transformer.py:270: UserWarning: CuDNN attention kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:723.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "/opt/sam2/sam2/modeling/sam/transformer.py:270: UserWarning: The CuDNN backend needs to be enabled by setting the enviornment variable`TORCH_CUDNN_SDPA_ENABLED=1` (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:497.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "/opt/sam2_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1562: UserWarning: Flash Attention kernel failed due to: No available kernel. Aborting execution.\n",
      "Falling back to all available kernels for scaled_dot_product_attention (which may have a slower speed).\n",
      "  return forward_call(*args, **kwargs)\n",
      "/opt/sam2/sam2/sam2_video_predictor_fsmedsam2.py:878: UserWarning: cannot import name '_C' from 'sam2' (/opt/sam2/sam2/__init__.py)\n",
      "\n",
      "Skipping the post-processing step due to the error above. You can still use SAM 2 and it's OK to ignore the error above, although some post-processing functionality may be limited (which doesn't affect the results in most cases; see https://github.com/facebookresearch/segment-anything-2/blob/main/INSTALL.md).\n",
      "  pred_masks_gpu = fill_holes_in_mask_scores(\n",
      "sam2_hiera_t.yaml\n",
      "label: 1, dice: 0.7869423286180631\n",
      "sam2_hiera_s.yaml\n",
      "label: 1, dice: 0.8767743306417339\n",
      "sam2_hiera_b+.yaml\n",
      "label: 1, dice: 0.8735482777134138\n",
      "sam2_hiera_l.yaml\n",
      "label: 1, dice: 0.8388133988151607\n"
     ]
    }
   ],
   "source": [
    "!python /opt/sam2/notebooks/infer_fsmedsam2_by_slice.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "/opt/sam2/sam2/modeling/sam/transformer.py:270: UserWarning: Memory efficient kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:719.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "/opt/sam2/sam2/modeling/sam/transformer.py:270: UserWarning: Memory Efficient attention has been runtime disabled. (Triggered internally at ../aten/src/ATen/native/transformers/sdp_utils_cpp.h:495.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "/opt/sam2/sam2/modeling/sam/transformer.py:270: UserWarning: Flash attention kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:721.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "/opt/sam2/sam2/modeling/sam/transformer.py:270: UserWarning: Flash attention was not compiled for current AMD GPU architecture. Attempting to run on architecture gfx1030 (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:193.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "/opt/sam2/sam2/modeling/sam/transformer.py:270: UserWarning: CuDNN attention kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:723.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "/opt/sam2/sam2/modeling/sam/transformer.py:270: UserWarning: The CuDNN backend needs to be enabled by setting the enviornment variable`TORCH_CUDNN_SDPA_ENABLED=1` (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:497.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "/opt/sam2_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1562: UserWarning: Flash Attention kernel failed due to: No available kernel. Aborting execution.\n",
      "Falling back to all available kernels for scaled_dot_product_attention (which may have a slower speed).\n",
      "  return forward_call(*args, **kwargs)\n",
      "/opt/sam2/sam2/sam2_video_predictor_fsmedsam2.py:878: UserWarning: cannot import name '_C' from 'sam2' (/opt/sam2/sam2/__init__.py)\n",
      "\n",
      "Skipping the post-processing step due to the error above. You can still use SAM 2 and it's OK to ignore the error above, although some post-processing functionality may be limited (which doesn't affect the results in most cases; see https://github.com/facebookresearch/segment-anything-2/blob/main/INSTALL.md).\n",
      "  pred_masks_gpu = fill_holes_in_mask_scores(\n",
      "sam2_hiera_t.yaml\n",
      "label: 1, dice: 0.0026476894066592618\n",
      "sam2_hiera_s.yaml\n",
      "label: 1, dice: 0.002814331242932045\n",
      "sam2_hiera_b+.yaml\n",
      "label: 1, dice: 0.037976210305038796\n",
      "sam2_hiera_l.yaml\n",
      "label: 1, dice: 0.9012917153967365\n"
     ]
    }
   ],
   "source": [
    "!python /opt/sam2/notebooks/infer_fsmedsam2_by_volume.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "/opt/sam2/sam2/modeling/sam/transformer.py:270: UserWarning: Memory efficient kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:719.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "/opt/sam2/sam2/modeling/sam/transformer.py:270: UserWarning: Memory Efficient attention has been runtime disabled. (Triggered internally at ../aten/src/ATen/native/transformers/sdp_utils_cpp.h:495.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "/opt/sam2/sam2/modeling/sam/transformer.py:270: UserWarning: Flash attention kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:721.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "/opt/sam2/sam2/modeling/sam/transformer.py:270: UserWarning: Flash attention was not compiled for current AMD GPU architecture. Attempting to run on architecture gfx1030 (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:193.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "/opt/sam2/sam2/modeling/sam/transformer.py:270: UserWarning: CuDNN attention kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:723.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "/opt/sam2/sam2/modeling/sam/transformer.py:270: UserWarning: The CuDNN backend needs to be enabled by setting the enviornment variable`TORCH_CUDNN_SDPA_ENABLED=1` (Triggered internally at ../aten/src/ATen/native/transformers/hip/sdp_utils.cpp:497.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "/opt/sam2_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1562: UserWarning: Flash Attention kernel failed due to: No available kernel. Aborting execution.\n",
      "Falling back to all available kernels for scaled_dot_product_attention (which may have a slower speed).\n",
      "  return forward_call(*args, **kwargs)\n",
      "/opt/sam2/sam2/sam2_video_predictor_fsmedsam2.py:878: UserWarning: cannot import name '_C' from 'sam2' (/opt/sam2/sam2/__init__.py)\n",
      "\n",
      "Skipping the post-processing step due to the error above. You can still use SAM 2 and it's OK to ignore the error above, although some post-processing functionality may be limited (which doesn't affect the results in most cases; see https://github.com/facebookresearch/segment-anything-2/blob/main/INSTALL.md).\n",
      "  pred_masks_gpu = fill_holes_in_mask_scores(\n",
      "sam2_hiera_t.yaml\n",
      "label: 1, dice: 0.1542735604440249\n",
      "sam2_hiera_s.yaml\n",
      "label: 1, dice: 0.7944910374354736\n",
      "sam2_hiera_b+.yaml\n",
      "label: 1, dice: 0.7217317762222663\n",
      "sam2_hiera_l.yaml\n",
      "label: 1, dice: 0.6576740985617375\n"
     ]
    }
   ],
   "source": [
    "!python /opt/sam2/notebooks/infer_fsmedsam2_by_volume_from_middle.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
