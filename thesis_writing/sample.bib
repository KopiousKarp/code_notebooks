@article{Hostetler2022,
  title = {Multiple brace root phenotypes promote anchorage and limit root lodging in maize},
  volume = {45},
  ISSN = {1365-3040},
  url = {http://dx.doi.org/10.1111/pce.14289},
  DOI = {10.1111/pce.14289},
  number = {5},
  journal = {Plant,  Cell \&amp; Environment},
  publisher = {Wiley},
  author = {Hostetler,  Ashley N. and Erndwein,  Lindsay and Reneau,  Jonathan W. and Stager,  Adam and Tanner,  Herbert G. and Cook,  Douglas and Sparks,  Erin E.},
  year = {2022},
  month = feb,
  pages = {1573–1583}
}

@ARTICLE{Seethepalli2021,
  title     = "{RhizoVision} Explorer: open-source software for root image
               analysis and measurement standardization",
  author    = "Seethepalli, Anand and Dhakal, Kundan and Griffiths, Marcus and
               Guo, Haichao and Freschet, Gregoire T and York, Larry M",
  abstract  = "Roots are central to the function of natural and agricultural
               ecosystems by driving plant acquisition of soil resources and
               influencing the carbon cycle. Root characteristics like length,
               diameter and volume are critical to measure to understand plant
               and soil functions. RhizoVision Explorer is an open-source
               software designed to enable researchers interested in roots by
               providing an easy-to-use interface, fast image processing and
               reliable measurements. The default broken roots mode is intended
               for roots sampled from pots and soil cores, washed and typically
               scanned on a flatbed scanner, and provides measurements like
               length, diameter and volume. The optional whole root mode for
               complete root systems or root crowns provides additional
               measurements such as angles, root depth and convex hull. Both
               modes support providing measurements grouped by defined diameter
               ranges, the inclusion of multiple regions of interest and batch
               analysis. RhizoVision Explorer was successfully validated
               against ground truth data using a new copper wire image set. In
               comparison, the current reference software, the commercial
               WinRhizo™, drastically underestimated volume when wires of
               different diameters were in the same image. Additionally,
               measurements were compared with WinRhizo™ and IJ\_Rhizo using a
               simulated root image set, showing general agreement in software
               measurements, except for root volume. Finally, scanned root
               image sets acquired in different labs for the crop, herbaceous
               and tree species were used to compare results from RhizoVision
               Explorer with WinRhizo™. The two software showed general
               agreement, except that WinRhizo™ substantially underestimated
               root volume relative to RhizoVision Explorer. In the current
               context of rapidly growing interest in root science, RhizoVision
               Explorer intends to become a reference software, improve the
               overall accuracy and replicability of root trait measurements
               and provide a foundation for collaborative improvement and
               reliable access to all.",
  journal   = "AoB Plants",
  publisher = "Oxford University Press (OUP)",
  volume    =  13,
  number    =  6,
  pages     = "lab056",
  month     =  dec,
  year      =  2021,
  keywords  = "Ground truth; phenomics; phenotyping; rhizosphere; root system
               architecture; traits",
  copyright = "https://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@article{Smith2022,
  title = {R<scp>oot</scp>P<scp>ainter</scp>: deep learning segmentation of biological images with corrective annotation},
  volume = {236},
  ISSN = {1469-8137},
  url = {http://dx.doi.org/10.1111/nph.18387},
  DOI = {10.1111/nph.18387},
  number = {2},
  journal = {New Phytologist},
  publisher = {Wiley},
  author = {Smith,  Abraham George and Han,  Eusun and Petersen,  Jens and Olsen,  Niels Alvin Faircloth and Giese,  Christian and Athmann,  Miriam and Dresbøll,  Dorte Bodin and Thorup‐Kristensen,  Kristian},
  year = {2022},
  month = aug,
  pages = {774–791}
}

@ARTICLE{Kirillov2023-ok,
  title         = "Segment Anything",
  author        = "Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and
                   Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao,
                   Tete and Whitehead, Spencer and Berg, Alexander C and Lo,
                   Wan-Yen and Doll{\'a}r, Piotr and Girshick, Ross",
  abstract      = "We introduce the Segment Anything (SA) project: a new task,
                   model, and dataset for image segmentation. Using our
                   efficient model in a data collection loop, we built the
                   largest segmentation dataset to date (by far), with over 1
                   billion masks on 11M licensed and privacy respecting images.
                   The model is designed and trained to be promptable, so it
                   can transfer zero-shot to new image distributions and tasks.
                   We evaluate its capabilities on numerous tasks and find that
                   its zero-shot performance is impressive -- often competitive
                   with or even superior to prior fully supervised results. We
                   are releasing the Segment Anything Model (SAM) and
                   corresponding dataset (SA-1B) of 1B masks and 11M images at
                   https://segment-anything.com to foster research into
                   foundation models for computer vision.",
  month         =  apr,
  year          =  2023,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2304.02643"
}


@Article{rs16030526,
AUTHOR = {Kim, Jiyong and Kim, Yongil},
TITLE = {Integrated Framework for Unsupervised Building Segmentation with Segment Anything Model-Based Pseudo-Labeling and Weakly Supervised Learning},
JOURNAL = {Remote Sensing},
VOLUME = {16},
YEAR = {2024},
NUMBER = {3},
ARTICLE-NUMBER = {526},
URL = {https://www.mdpi.com/2072-4292/16/3/526},
ISSN = {2072-4292},
ABSTRACT = {The Segment Anything Model (SAM) has had a profound impact on deep learning applications in remote sensing. SAM, which serves as a prompt-based foundation model for segmentation, exhibits a remarkable capability to “segment anything,” including building objects on satellite or airborne images. To facilitate building segmentation without inducing supplementary prompts or labels, we applied a sequential approach of generating pseudo-labels and incorporating an edge-driven model. We first segmented the entire scene by SAM and masked out unwanted objects to generate pseudo-labels. Subsequently, we employed an edge-driven model designed to enhance the pseudo-label by using edge information to reconstruct the imperfect building features. Our model simultaneously utilizes spectral features from SAM-oriented building pseudo-labels and edge features from resultant images from the Canny edge detector and, thus, when combined with conditional random fields (CRFs), shows capability to extract and learn building features from imperfect pseudo-labels. By integrating the SAM-based pseudo-label with our edge-driven model, we establish an unsupervised framework for building segmentation that operates without explicit labels. Our model excels in extracting buildings compared with other state-of-the-art unsupervised segmentation models and even outperforms supervised models when trained in a fully supervised manner. This achievement demonstrates the potential of our model to address the lack of datasets in various remote sensing domains for building segmentation.},
DOI = {10.3390/rs16030526}
}
@misc{AlterNet,
  doi = {10.48550/ARXIV.2202.06709},
  url = {https://arxiv.org/abs/2202.06709},
  author = {Park,  Namuk and Kim,  Songkuk},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),  Machine Learning (cs.LG),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {How Do Vision Transformers Work?},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16000--16009},
  year={2022}
}
@misc{Ates_2023, title={Feed grains sector at a glance}, url={https://www.ers.usda.gov/topics/crops/corn-and-other-feed-grains/feed-grains-sector-at-a-glance/}, journal={USDA ERS - Feed Grains Sector at a Glance}, publisher={USDA}, author={Ates, Aaron  M.}, year={2023}, month={Dec}} 

@book{AgOutlook2019,
  title = {OECD-FAO Agricultural Outlook 2019-2028},
  ISBN = {9789264312463},
  ISSN = {1999-1142},
  url = {http://dx.doi.org/10.1787/agr_outlook-2019-en},
  DOI = {10.1787/agr_outlook-2019-en},
  journal = {OECD-FAO Agricultural Outlook},
  publisher = {OECD},
  year = {2019},
  month = jul 
}

@misc{Blizard2020,
  title = {Maize Nodal Roots},
  ISBN = {9781119312994},
  url = {http://dx.doi.org/10.1002/9781119312994.apr0735},
  DOI = {10.1002/9781119312994.apr0735},
  journal = {Annual Plant Reviews online},
  publisher = {Wiley},
  author = {Blizard,  Sarah and Sparks,  Erin E.},
  year = {2020},
  month = may,
  pages = {281–304}
}

@article{Reneau2020,
  title = {Maize brace roots provide stalk anchorage},
  volume = {4},
  ISSN = {2475-4455},
  url = {http://dx.doi.org/10.1002/pld3.284},
  DOI = {10.1002/pld3.284},
  number = {11},
  journal = {Plant Direct},
  publisher = {Wiley},
  author = {Reneau,  Jonathan W. and Khangura,  Rajdeep S. and Stager,  Adam and Erndwein,  Lindsay and Weldekidan,  Teclemariam and Cook,  Douglas D. and Dilkes,  Brian P. and Sparks,  Erin E.},
  year = {2020},
  month = nov 
}

@misc{Park22,
  doi = {10.48550/ARXIV.2202.06709},
  url = {https://arxiv.org/abs/2202.06709},
  author = {Park,  Namuk and Kim,  Songkuk},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),  Machine Learning (cs.LG),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {How Do Vision Transformers Work?},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}
@article{plantScienceAnnRev24,
   author = "Murphy, Katherine M. and Ludwig, Ella and Gutierrez, Jorge and Gehan, Malia A.",
   title = "Deep Learning in Image-Based Plant Phenotyping", 
   journal= "Annual Review of Plant Biology",
   year = "2024",
   volume = "75",
   number = "Volume 75, 2024",
   pages = "771-795",
   doi = "https://doi.org/10.1146/annurev-arplant-070523-042828",
   url = "https://www.annualreviews.org/content/journals/10.1146/annurev-arplant-070523-042828",
   publisher = "Annual Reviews",
   issn = "1545-2123",
   type = "Journal Article",
   keywords = "artificial intelligence",
   keywords = "machine learning",
   keywords = "deep learning",
   keywords = "imaging",
   keywords = "plant phenomics",
   keywords = "convolutional neural networks",
   keywords = "computer vision",
   abstract = "A major bottleneck in the crop improvement pipeline is our ability to phenotype crops quickly and efficiently. Image-based, high-throughput phenotyping has a number of advantages because it is nondestructive and reduces human labor, but a new challenge arises in extracting meaningful information from large quantities of image data. Deep learning, a type of artificial intelligence, is an approach used to analyze image data and make predictions on unseen images that ultimately reduces the need for human input in computation. Here, we review the basics of deep learning, assessments of deep learning success, examples of applications of deep learning in plant phenomics, best practices, and open challenges.",
  }

@misc{bai2024fsmedsam2exploringpotentialsam2,
      title={FS-MedSAM2: Exploring the Potential of SAM2 for Few-Shot Medical Image Segmentation without Fine-tuning}, 
      author={Yunhao Bai and Qinji Yu and Boxiang Yun and Dakai Jin and Yingda Xia and Yan Wang},
      year={2024},
      eprint={2409.04298},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.04298}, 
}

@article{Madec2023,
  title = {VegAnn,  Vegetation Annotation of multi-crop RGB images acquired under diverse conditions for segmentation},
  volume = {10},
  ISSN = {2052-4463},
  url = {http://dx.doi.org/10.1038/s41597-023-02098-y},
  DOI = {10.1038/s41597-023-02098-y},
  number = {1},
  journal = {Scientific Data},
  publisher = {Springer Science and Business Media LLC},
  author = {Madec,  Simon and Irfan,  Kamran and Velumani,  Kaaviya and Baret,  Frederic and David,  Etienne and Daubige,  Gaetan and Samatan,  Lucas Bernigaud and Serouart,  Mario and Smith,  Daniel and James,  Chrisbin and Camacho,  Fernando and Guo,  Wei and De Solan,  Benoit and Chapman,  Scott C. and Weiss,  Marie},
  year = {2023},
  month = may 
}

@article{ravi2024sam2,
  title={SAM 2: Segment Anything in Images and Videos},
  author={Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Doll{\'a}r, Piotr and Feichtenhofer, Christoph},
  journal={arXiv preprint arXiv:2408.00714},
  url={https://arxiv.org/abs/2408.00714},
  year={2024}
}
@misc{FS_medSAM2,
  doi = {10.48550/ARXIV.2409.04298},
  url = {https://arxiv.org/abs/2409.04298},
  author = {Bai,  Yunhao and Yu,  Qinji and Yun,  Boxiang and Jin,  Dakai and Xia,  Yingda and Wang,  Yan},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {FS-MedSAM2: Exploring the Potential of SAM2 for Few-Shot Medical Image Segmentation without Fine-tuning},
  publisher = {arXiv},
  year = {2024},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@article{DIRT3D,
title={DIRT/3D: 3D phenotyping for field-grown maize (Zea mays)},
author={Das, Abhiram and Schneider, Hannah and Burridge, James and Ascanio, Ana Karine Martinez and Wojciechowski, Tobias and Topp, Christopher N and Lynch, Jonathan P and Weitz, Joshua S and Bucksch, Alexander},
journal={Plant physiology},
volume={187},
number={3},
pages={739--757},
year={2021},
publisher={Oxford University Press}
}

@article{FaRIA,
title={Fully-automated root image analysis (faRIA)},
author={Narisetti, Nihar and Henke, Michael and Seiler, Christian and Shi, Ruonan and Junker, Astrid and Altmann, Thomas and Gladilin, Evgeny},
journal={Scientific reports},
volume={11},
number={1},
pages={16132},
year={2021},
publisher={Nature Publishing Group}
}

@article{Tang2024,
  title = {GRABSEEDS: extraction of plant organ traits through image analysis},
  volume = {20},
  ISSN = {1746-4811},
  url = {http://dx.doi.org/10.1186/s13007-024-01268-2},
  DOI = {10.1186/s13007-024-01268-2},
  number = {1},
  journal = {Plant Methods},
  publisher = {Springer Science and Business Media LLC},
  author = {Tang,  Haibao and Kong,  Wenqian and Nabukalu,  Pheonah and Lomas,  Johnathan S. and Moser,  Michel and Zhang,  Jisen and Jiang,  Mengwei and Zhang,  Xingtan and Paterson,  Andrew H. and Yim,  Won Cheol},
  year = {2024},
  month = sep 
}
